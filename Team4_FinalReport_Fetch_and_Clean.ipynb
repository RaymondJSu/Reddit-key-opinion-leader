{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden wind in r/WallStreetBets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Pushshift API to fetch data\n",
    "\n",
    "The PSAW library (different from the `praw` library!) lets you access this data resource as well: [PushShift.io API Wrapper](https://github.com/dmarx/psaw)\n",
    "\n",
    "* r/WallStreetBets Research Project Phase 3\n",
    "* Team 4\n",
    "* Anqi Fang, Jiashuo Sun, Raymond Su\n",
    "\n",
    "**This is a note book for part 2 of our analysis - Data preparation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()\n",
    "# Define some keys for submission attribtues\n",
    "filter_keys = ['author','id','num_comments',\n",
    "               'score','upvote_ratio','title','selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling dates and times\n",
    "from datetime import datetime\n",
    "#also import pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the starting and ending time frame\n",
    "\n",
    "start = int(datetime(2020, 12, 8).timestamp())\n",
    "end = int(datetime(2021, 3, 1).timestamp())\n",
    "\n",
    "search = api.search_submissions(after=start,\n",
    "                                before = end,\n",
    "                                subreddit='wallstreetbets',\n",
    "                                filter=filter_keys,\n",
    "                                sort='asc',\n",
    "                                limit=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for the results\n",
    "all_subs = []\n",
    "\n",
    "# Loop through the search results to actually get data\n",
    "for i,sub in enumerate(search):\n",
    "    \n",
    "    # Add each result's dictionary (the .d_ attribute) to the all_subs\n",
    "    all_subs.append(sub.d_)\n",
    "    \n",
    "    # Print out status updates every 10,000 submissions\n",
    "    if i % 10000 == 0:\n",
    "        \n",
    "        # The current time so you know how long in between updates\n",
    "        time_now = datetime.now().time().replace(microsecond=0)\n",
    "        \n",
    "        # The date of the submission to give you an idea of how far along you are\n",
    "        record_date = datetime.utcfromtimestamp(sub.d_['created']).date()\n",
    "        \n",
    "        # Print it out\n",
    "        print(\"{0:,} for {1} received at {2}\".format(i,record_date,time_now))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "837,282\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>created</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Zckary</td>\n",
       "      <td>1607414431</td>\n",
       "      <td>k90ai4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19 year old college student in debt looking fo...</td>\n",
       "      <td>SERIOUS INQUIRIES ONLY</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.607440e+09</td>\n",
       "      <td>2020-12-08 15:00:31</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginityisgood</td>\n",
       "      <td>1607414453</td>\n",
       "      <td>k90ao2</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>Money isnâ€™t real, will make everything back th...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.607440e+09</td>\n",
       "      <td>2020-12-08 15:00:53</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shogunz888</td>\n",
       "      <td>1607414850</td>\n",
       "      <td>k90dkb</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>TSLA to the moon! ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.607440e+09</td>\n",
       "      <td>2020-12-08 15:07:30</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BelmontMan</td>\n",
       "      <td>1607415309</td>\n",
       "      <td>k90gqc</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>Investing or gambling?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.607441e+09</td>\n",
       "      <td>2020-12-08 15:15:09</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yaplugxbl</td>\n",
       "      <td>1607415449</td>\n",
       "      <td>k90hmz</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>SNDL?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.607441e+09</td>\n",
       "      <td>2020-12-08 15:17:29</td>\n",
       "      <td>2020-12-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  created_utc      id  num_comments  score  \\\n",
       "0           Zckary   1607414431  k90ai4             3      1   \n",
       "1  virginityisgood   1607414453  k90ao2            46      1   \n",
       "2       shogunz888   1607414850  k90dkb            17      1   \n",
       "3       BelmontMan   1607415309  k90gqc             0      1   \n",
       "4        yaplugxbl   1607415449  k90hmz             0      1   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  19 year old college student in debt looking fo...   \n",
       "1                                                      \n",
       "2                                                      \n",
       "3                                          [removed]   \n",
       "4                                          [removed]   \n",
       "\n",
       "                                               title  upvote_ratio  \\\n",
       "0                             SERIOUS INQUIRIES ONLY           1.0   \n",
       "1  Money isnâ€™t real, will make everything back th...           1.0   \n",
       "2                            TSLA to the moon! ðŸš€ðŸš€ðŸš€ðŸš€ðŸš€           1.0   \n",
       "3                             Investing or gambling?           1.0   \n",
       "4                                              SNDL?           1.0   \n",
       "\n",
       "        created           timestamp        date  \n",
       "0  1.607440e+09 2020-12-08 15:00:31  2020-12-08  \n",
       "1  1.607440e+09 2020-12-08 15:00:53  2020-12-08  \n",
       "2  1.607440e+09 2020-12-08 15:07:30  2020-12-08  \n",
       "3  1.607441e+09 2020-12-08 15:15:09  2020-12-08  \n",
       "4  1.607441e+09 2020-12-08 15:17:29  2020-12-08  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# turn list of submission into DataFrame\n",
    "subs_df = pd.DataFrame(all_subs)\n",
    "print('{:,}'.format(len(all_subs)))\n",
    "\n",
    "# translate Unix time to understandable format\n",
    "subs_df['timestamp'] = subs_df['created'].apply(datetime.utcfromtimestamp)\n",
    "subs_df['date'] = subs_df['timestamp'].apply(lambda x:x.date())\n",
    "\n",
    "# save data as csv file\n",
    "subs_df.to_csv('all_submissions.csv',encoding='utf8',index=False)\n",
    "\n",
    "subs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, io\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### check the missing values in all columns\n",
    "subs_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop the missing values\n",
    "subs_df = subs_df[subs_df.selftext.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I checked random rows in this dataset, I found some rows containing \"removed\" \"deleted\" as the text value of the reddit submission. So I added a filter to drop those rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove the rows with \"removed\" as the text value\n",
    "subs_df = subs_df[subs_df.selftext != \"[removed]\"]\n",
    "subs_df = subs_df[subs_df.selftext != \"[deleted]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = subs_df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### clean the text\n",
    "def textcleaner(row):\n",
    "    # convert every word to lower case\n",
    "    row = row.lower()\n",
    "    #remove urls\n",
    "    row  = re.sub(r'http\\S+', '', row)\n",
    "    #remove mentions\n",
    "    row = re.sub(r\"(?<![@\\w])@(\\w{1,25})\", '', row)\n",
    "    #remove hashtags\n",
    "    row = re.sub(r\"(?<![#\\w])#(\\w{1,25})\", '',row)\n",
    "    #remove other special characters\n",
    "    row = re.sub('[^A-Za-z .-]+', '', row)\n",
    "    ### remove the \".\" and \"-\"\n",
    "    row  = re.sub('[.-]', '', row)\n",
    "    #remove digits\n",
    "    row = re.sub('\\d+', '', row)\n",
    "    row = row.strip(\" \")\n",
    "    row = re.sub('\\s+', ' ', row)\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_textlist = []\n",
    "### clean the text in the list\n",
    "for t in text_list:\n",
    "    cleaned_textlist.append(textcleaner(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides the general text cleaning, we also want to remove the stopwords, which do not carry contextual meanings in themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### remove stopwords\n",
    "stopeng = set(stopwords.words('english'))\n",
    "\n",
    "cleaned = []\n",
    "for row in cleaned_textlist:\n",
    "    tokens = word_tokenize(row)\n",
    "    tokens_nostop = [w for w in tokens if w not in stopeng]\n",
    "    cleaned_text = \" \".join(tokens_nostop)\n",
    "    cleaned.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs_df['cleaned_text'] = cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to tokenize the text, I choose the function word_tokenize() because the text has already been processed and cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### tokenize the text\n",
    "token_result = []\n",
    "\n",
    "for item in cleaned:\n",
    "    tokened = word_tokenize(item)\n",
    "    token_result.append(tokened)\n",
    "subs_df['tokened_text'] = token_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### save the results as a csv file\n",
    "subs_df.to_csv(\"PATH/cleaned_submissions.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
